from typing import Tuple
from ExpectedUtility import ExpectedUtility

from PriorityQueue import PriorityQueue
from DataTypes.Schedule import Schedule
from DataTypes.Transform import Transform
from SearchStrategies.BestFirstSearch import best_first_search
from parse_files import ParseFiles
from DataTypes.State import State
from DataTypes.Action import Action
from successors import RandomSuccessorFunction


def country_scheduler(self_country_name: str, resources_filename: str, initial_world_state: dict,
                      output_filename: str, num_output_schedulers: int, depth_bound: int,
                      max_frontier_size: int) -> None:

    resource_weights = ParseFiles.parse_resources_weights()
    resource_proportions = ParseFiles.parse_resources_proportions()

    ######################################
    # Calculate initial state quality
    ######################################

    country_world_state = initial_world_state[self_country_name]
    self_state_quality_start = ExpectedUtility.state_quality_for_country(
        country_world_state, resource_weights, resource_proportions)
    print(f"My country's initial state: {country_world_state}")

    self_state_quality_end = ExpectedUtility.state_quality_for_country(
        country_world_state, resource_weights, resource_proportions)
    print(f"Starting State Quality: {self_state_quality_start}")
    print(f"Country's state after transform: {country_world_state}")
    print(f"Ending State Quality: {self_state_quality_end}")

    # Calculate Discounted Reward

    # Test - calculate Discounted Reward - should be -0.03125
    q_start = self_state_quality_start
    q_end = self_state_quality_end
    gamma = 0.5
    n = 5  # steps to get into state

    discounted_reward = ExpectedUtility.discounted_reward(
        gamma, n, q_end, q_start)
    print(f"Discounted Reward for {self_country_name}: {discounted_reward}")

    # calculate schedule EU

    # Probability of schedule acceptance
    # Equation: we will use the product of the probabilities of the individual P(ci,sj) value
    #   - The probability that a country, ci, will agree to a schedule, sj, is computed by the logistic function where x corresponds to DR(ci, sj ) and L = 1, and you can experiment with different values of x0 and k (but use x0 = 0 and k = 1 as starting points)
    # TODO: Think about and report on how different parameter settings might reflect biases in the real world (e.g., a reason for shifting x0 might be to reflect opportunity costs — what other benefits might await a patient country?).
    # Note that the strategy for estimating the probability that a schedule is accepted (i.e., will be accepted by all parties to the schedule) and succeeds, does not come from statistics accu- mulated over data from the “real world” (to include game play) as we might think is ideal, but our method draws from an information theoretic tradition of estimating probabilities of “events” from the “descriptions” of those events. An assumption in this description- driven methodology is a bias that more “complicated” descriptions represent lower proba- bility events. It is a quantification of Occam’s razor. This is the one way (probably the only way given the time constraints of this class) that the AI agent for your country will take into account the State Qualities of other countries, as well as your own. It has the effect of countering ill-considered greed.

    # todo - move rest to classes so easier. Gather participating countries from schedule, or check if SQ changed (Live 8 3:00)
    participating_countries = ["Dinotopia", "Atlantis"]
    sigmoid_midpoint = 1  # todo is this right?
    x0 = discounted_reward - sigmoid_midpoint

    # TODO: tweak
    # Probability of country's acceptance of a schedule (using logistic function)
    # k = 1  # steepness of the curve
    # L = 1  # max value of the curve
    p = ExpectedUtility.logistic_function(x0)  # x0 is midpoint of curve

    # Multiply them for overall schedule acceptance -
    for country in participating_countries:
        p += p * ExpectedUtility.logistic_function(x0)
    print(f"Probability of schedule acceptance: {p}")

    # Expected Utility calculation
    c = 0.1
    eu = ExpectedUtility.expected_utility(p, discounted_reward, c)
    print(f"Expected Utility: {eu}")

    # # generate successor by adding new random schedule

    # Define utility function
    # TODO
    # def utility_fn(state: State) -> float:
    #     return state.get_utility()

    # Define search strategy
    # TODO - use my own, what when where?
    """ 
    Best-first search?
    Heuristic depth-first search?
    Iterative deepening heuristic depth-first search?
    """
    def successor_fn():
        """
        Returns a list of successor states generated by the generate_random_schedule function.
        """

        sch_gen = RandomSuccessorFunction(self_country_name)

        successors = []
        for _ in range(10):  # Generate 10 successors
            successor = sch_gen.generate_random_schedule(5)
            successors.append(successor)
        return successors

    best_first_search(initial_world_state, successor_fn,
                      depth_bound, max_frontier_size)

    # # Search for optimal schedule
    # solution = search_strategy.search(
    #     initial_world_state, schedule.ac, depth_bound, max_frontier_size)

    # todo: depth bound

    ######################################
    # write schedule to file
    ######################################
    # with open(output_filename, 'w') as f:
    #     writer = csv.writer(f)
    #     for i in range(num_output_schedulers):
    # writer.writerow(solution.get_schedule(i))
    # for filename in os.listdir('transformation-templates', 'r'):
    #     template_path = os.path.join(templates_path, filename)
    #     with open(template_path, 'r') as file:
    #         contents = file.read().strip()
    #         transform = Transform(template_path, self.logger)
    #         actions.append(transform_action)
